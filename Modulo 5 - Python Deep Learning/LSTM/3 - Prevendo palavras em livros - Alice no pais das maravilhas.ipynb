{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "120a1903",
   "metadata": {},
   "source": [
    "### Imitando o estilo de escrita de um autor(a) com rede LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46027b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image #Para visualizar imagens explicativas\n",
    "Image(filename='Alice.jpg') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac9bc76",
   "metadata": {},
   "source": [
    "Será que conseguimos imitar o estilo de escrita de um autor(a)? \n",
    "\n",
    "Vamos aprender como uma rede LSTM pode nos ajudar a prever palavras com base em últimas sequências."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa12bb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf, numpy as np \n",
    "from tensorflow import keras \n",
    "from keras import layers \n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import re, string # expressões regulares\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2dbd97",
   "metadata": {},
   "source": [
    "Iniciaremos lendo o arquivo inteiro em uma unica string para removermos pontuação. \n",
    "\n",
    "Feito isso adicionamos um espaço após cada quebra de linha “\\n” para que a divisão das palavras em text.split() considere a quebra de linha como uma palavra (Fazemos isso para que a rede use essa palavra “quebra de linha” quando decidir iniciar uma nova linha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c87095",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('wonderland.txt', errors=\"ignore\").read() #lendo o arquivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af18bf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30686233",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66d58cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.lower()                                   # somente letras minusculas\n",
    "text = text.replace('\\n', ' \\n ')                     # considerar \\n (espaço) como palavra\n",
    "text_words = [w for w in text.split(' ') if w.strip() != '' or w == '\\n']\n",
    "text_words = [re.sub(r'[^\\x00-\\x7f]',r'', s) for s in text_words] # remove numeros e acentos\n",
    "print('Quantidade Total de Palavras: ', len(text_words))\n",
    "print(text_words[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32255dcf",
   "metadata": {},
   "source": [
    "### Criando um dicionário de palavras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c35bb8",
   "metadata": {},
   "source": [
    "Tendo agora as palavras em uma **grande lista**, usamos a classe **Tokenizer** para tokenização das palavras e criação de um vocabulário com as 500 palavras mais frequentes no texto(parâmetro num_words do Tokenizer):\n",
    "\n",
    "**Tokenizar** é a tarefa de cortar um texto em pedaços chamados tokens e, ao mesmo tempo, jogar fora alguns caracteres não úteis, como por exemplo pontuação. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c9f3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 200 # tamanho do vocabulário\n",
    "\n",
    "# criando tokenizer (usa somente as 'VOCAB_SIZE' palavras mais comuns)\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>', #<OOV> palavras que não estão no dicionário\n",
    "                      filters='[^\\x00-\\x7f]')\n",
    "\n",
    "# definindo vocabulario\n",
    "tokenizer.fit_on_texts(text_words)\n",
    "\n",
    "# tokenizando as palavras\n",
    "tokens = tokenizer.texts_to_sequences(text_words)\n",
    "tokens = [int(t[0]) for t in tokens] # lista para inteiros\n",
    "print(text_words[:10])\n",
    "print(tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83d9828",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_words[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddd1005",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480bd81b",
   "metadata": {},
   "source": [
    "### Definindo no alvo:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85990cf3",
   "metadata": {},
   "source": [
    "Após tokenizar as sentenças, devemos agrupar as palavras para criar as sub-sequencias de entrada(X) e as respectivas saídas, ou seja, as palavras seguintes(Y). \n",
    "\n",
    "Ao fatiar a lista podemos definir um intervalo entre cada sequência (step no código abaixo), isso reduz a produção de sequencias com palavras sobrepostas e repetidas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9687fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1 # distancia a cada fatia \n",
    "sentences = [] # frases X\n",
    "next_words = [] # palavras Y\n",
    "seq_len = 10 # número de tokens por frase\n",
    "for i in range(0, len(tokens) - seq_len, step):\n",
    "    # Only add sequences where no word is in ignored_words\n",
    "    sentences.append(tokens[i: i + seq_len])\n",
    "    next_words.append(tokens[i + seq_len])\n",
    "   \n",
    "print(f'[+] Instâncias para treino: {len(sentences)}')\n",
    "for i in range(5):\n",
    "    print(sentences[i],'-->',next_words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c97096",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_words[24]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa56057",
   "metadata": {},
   "source": [
    "O ultimo passo de preprocessamento é codificar cada token usando a estratégia one-hot. Após a codificação cada token será representado por um vetor de 500 dimensões (tamanho do vocabulário).\n",
    "\n",
    "Ao utilizar vetorização one-hot temos que cada palavra é representada por um vetor do tamanho do vocabulário onde somente uma posição é = 1 (“hot”). Como vamos utilizar redes recorrentes com camadas LSTM, usaremos uma sequência de palavras como entrada, ao invés de uma única palavra. \n",
    "\n",
    "A camada LSTM itera sobre a sequência de palavras produzindo saídas, porém somente a ultima saída é usada para alimentar as ultimas camadas (return_sequences=False). Em seguida temos uma camada densa que aplica a função."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedf99a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='OneHot.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c1900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostra shape atual\n",
    "train_x = np.array(sentences) # pega as sentenças\n",
    "train_y = np.array(next_words) # pega as palavras seguintes\n",
    "print('Shape X:',train_x.shape)\n",
    "print('Shape Y:',train_y.shape)\n",
    "\n",
    "# codificando para one-hot\n",
    "train_x_onehot = to_categorical(train_x, num_classes=vocab_size)\n",
    "train_y_onehot = to_categorical(train_y, num_classes=vocab_size)\n",
    "\n",
    "# train_x:(num_exemplos, num_tokens, vocab_size)\n",
    "print('Shape X após one-hot:',train_x_onehot.shape)\n",
    "\n",
    "# train_y:(num_exemplos,vocab_size)\n",
    "print('Shape Y após one-hot:',train_y_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6ed4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73287592",
   "metadata": {},
   "source": [
    "### Definição e Treino da Rede Recorrente LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc52aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b29848f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.LSTM(64, input_shape=(seq_len, vocab_size), return_sequences=False,), #entrada dos dados\n",
    "    layers.Dense(vocab_size, activation='softmax') #Cada unidade representa a probabilidade de cada palavra do vocabulário ser a próxima com base na sequencia de entrada.\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d386f8",
   "metadata": {},
   "source": [
    "Como a saída do modelo é uma lista com a distribuição de probabilidade de uma classificação multiclasse , é adequado utilizar a função de custo categorical_crossentropy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da12bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e031243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history = model.fit(train_x_onehot, train_y_onehot, validation_split=0.2,\n",
    "                    epochs=40, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ea7945",
   "metadata": {},
   "source": [
    "## Analisando a curva de aprendizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a11b96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_history.history['accuracy'], label='Treino')\n",
    "plt.plot(train_history.history['val_accuracy'], label='Validação')\n",
    "plt.xlabel('Epocas');plt.ylabel('Acurácia')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29afb36",
   "metadata": {},
   "source": [
    "Com certeza poderiamos aumentar as epocas de processamento e obter cada vez uma acurácia maior e um erro menor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8499849",
   "metadata": {},
   "source": [
    "### Escrevendo como  Lewis Carroll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84c779f",
   "metadata": {},
   "source": [
    "Após termos o modelo treinado podemos utiliza-lo para prever algumas palavras.\n",
    "\n",
    "Começaremos com a palavra seed “Alice:”. \n",
    "\n",
    "Precisamos que a sequencia de entrada tenha o tamanho seq_len definido, no nosso caso 10 palavras. Para isso utilizamos o auxiliar pad_sequences que insere espaços à esquerda da sequencia de forma a preencher as posições faltantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0fdfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = ['Alice:'] # frase inicial\n",
    "seed_tokens = tokenizer.texts_to_sequences(seed_text)[0] # substitui palavras por tokens\n",
    "\n",
    "# preenche sequencia com zeros para ter o comprimento adequado pra rede\n",
    "tokens_x = pad_sequences([seed_tokens], maxlen=seq_len, ) #10\n",
    "\n",
    "tokens_x = to_categorical(tokens_x, num_classes=vocab_size) # one hot\n",
    "pred_y = model.predict(tokens_x)[0] # preve probabilidades para a proxima palavra\n",
    "\n",
    "print(f'Quantidade de Probabilidades: {len(pred_y)}')\n",
    "print(pred_y[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fdebe1",
   "metadata": {},
   "source": [
    "Tendo as probabilidades de cada palavra em mãos podemos selecionar a palavra com maior probabilidade ou pegar uma outra palavra que possui uma das mais altas probabilidade. Como cada valor de pred_y é a probabilidade para cada índice do token, realizamos a conversão desse indice para a palavra, segundo o nosso dicionário:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b89f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pega indice da palavra com maior probabilidade \n",
    "next_token = np.argmax(pred_y,)\n",
    "\n",
    "# realiza a inversão de token para palavra\n",
    "next_word = tokenizer.sequences_to_texts([[next_token]])\n",
    "print('Proximo token: ', next_token, '-->', next_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e444a93e",
   "metadata": {},
   "source": [
    "Uma maneira mais interessante de escolher a próxima palavra é usar uma distribuição categórica para calcular o índice do próximo token. Para isso definiremos uma função sample_word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197549fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_word(pred_y, temperature=1.0):\n",
    "    pred_y = pred_y / temperature # 'força' das probabilidades\n",
    "    pred_token = tf.random.categorical(pred_y, 1).numpy()\n",
    "    return pred_token # token de saída"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7360cb34",
   "metadata": {},
   "source": [
    "Essa função toma como entrada a distribuição de probabilidades das palavras (saída da rede) e amostra um dos tokens com base nesses valores. Note que mesmo com a mesma entrada pred_y essa função retornará diferentes valores, devido a sua natureza probabilistica. Vamos testar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc13977",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = ['Alice:'] # frase inicial\n",
    "seed_tokens = tokenizer.texts_to_sequences(seed_text)[0] # substitui palavras por tokens\n",
    "print(seed_text, ' tokenizado fica: ', seed_tokens)\n",
    "\n",
    "# preenche sequencia com zeros para ter o comprimento adequado pra rede\n",
    "tokens_x = pad_sequences([seed_tokens], maxlen=seq_len, )\n",
    "\n",
    "tokens_x = to_categorical(tokens_x, num_classes=vocab_size) # one hot\n",
    "pred_y = model.predict(tokens_x) # preve probabilidades para a proxima palavra\n",
    "next_token = sample_word(pred_y)\n",
    "\n",
    "# realiza a inversão de token para palavra\n",
    "next_word = tokenizer.sequences_to_texts(next_token)\n",
    "print('Proximo token: ', next_token, '-->', next_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c6649e",
   "metadata": {},
   "source": [
    "Para gerar um texto completo, simplesmente colocamos o código acima dentro de um loop, anexando a palavra de saída à frase original, lembrando sempre de aplicar as mesmas etapas de pré-processamento realizadas nas frases do treino:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f60d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = ['Alice:'] # frase inicial\n",
    "next_words = 50 # 50 próximas palavras\n",
    "\n",
    "# substitui palavras por tokens\n",
    "seed_tokens = tokenizer.texts_to_sequences(seed_text)[0] # substitui palavras por tokens\n",
    "print(seed_text, ' tokenizado fica: ', seed_tokens)\n",
    "\n",
    "for _ in range(next_words):\n",
    "    # preenche sequencia com zeros para ter o comprimento adequado pra rede\n",
    "    tokens_x = pad_sequences([seed_tokens], maxlen=seq_len, )\n",
    "    # transforma tokens em vetor one-hot\n",
    "    tokens_x = to_categorical(tokens_x, num_classes=vocab_size) # one hot\n",
    "    # preve probabilidades para a proxima palavra\n",
    "    pred_y = model.predict(tokens_x)\n",
    "    # faz amostragem com base nas probabilidades\n",
    "    next_token = sample_word(pred_y, 0.2)\n",
    "    next_token = next_token.flatten()[0] # pega valor como um int\n",
    "    # anexa token a lista\n",
    "    seed_tokens.append(next_token) \n",
    "    \n",
    "# como a saída é um conjunto de tokens\n",
    "# realiza a inversão para palavras, usando word_index\n",
    "resultado = tokenizer.sequences_to_texts([seed_tokens])\n",
    "print('\\n')\n",
    "print(resultado[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
